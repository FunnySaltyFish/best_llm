[
  {
    "id": 1,
    "name": "LLaMA",
    "vendor": "Meta",
    "intro": "LLaMA-13B outperforms GPT-3(175B) and LLaMA-65B is competitive to PaLM-540M.<br />Base model for most follow-up works.",
    "region": "",
    "tags": [],
    "url": "https://github.com/facebookresearch/llama",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 2,
    "name": "llama.cpp",
    "vendor": "@ggerganov",
    "intro": "c/cpp implementation for llama and some other models, using quantization.",
    "region": "",
    "tags": [],
    "url": "https://github.com/ggerganov/llama.cpp",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 3,
    "name": "Alpaca",
    "vendor": "Stanford",
    "intro": "use 52K instruction-following data generated by Self-Instructt techniques to fine-tune 7B LLaMA,<br /> the resulting model,  Alpaca, behaves similarly to the `text-davinci-003` model on the Self-Instruct instruction-following evaluation suite.<br />Alpaca has inspired many follow-up models.",
    "region": "",
    "tags": [],
    "url": "https://github.com/tatsu-lab/stanford_alpaca",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 4,
    "name": "BELLE",
    "vendor": "LianJiaTech",
    "intro": "maybe the first Chinese model to follow Alpaca.",
    "region": "",
    "tags": [],
    "url": "https://github.com/LianjiaTech/BELLE",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 5,
    "name": "ChatGLM-6B",
    "vendor": "Tsinghua",
    "intro": "well-known Chinese model, in chat mode, and can run on single GPU.",
    "region": "",
    "tags": [],
    "url": "https://github.com/THUDM/ChatGLM-6B",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 6,
    "name": "Dolly",
    "vendor": "Databricks",
    "intro": "use Alpaca data to fine-tune a 2-year-old model: GPT-J, which exhibits surprisingly high quality<br /> instruction following behavior not characteristic of the foundation model on which it is based.",
    "region": "",
    "tags": [],
    "url": "https://github.com/databrickslabs/dolly",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 7,
    "name": "Alpaca-LoRA",
    "vendor": "@tloen",
    "intro": "trained within hours on a single RTX 4090,<br />reproducing the [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) results using [low-rank adaptation (LoRA)](https://arxiv.org/pdf/2106.09685.pdf),<br />and can run on a Raspberry pi.",
    "region": "",
    "tags": [],
    "url": "https://github.com/tloen/alpaca-lora",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 8,
    "name": "ColossalChat",
    "vendor": "ColossalAI",
    "intro": "provides a unified large language model framework, including:<br />Supervised datasets collection<br />Supervised instructions fine-tuning<br />Reward model training<br />RLHF<br />Quantization inference<br />Fast model deploying<br />Perfectly integrated with the Hugging Face ecosystem",
    "region": "",
    "tags": [],
    "url": "https://github.com/hpcaitech/ColossalAI/blob/main/applications/Chat/README.md",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 9,
    "name": "LLaMA-Adapter",
    "vendor": "Shanghai AI Lab",
    "intro": "Fine-tuning LLaMA to follow instructions within 1 Hour and 1.2M Parameters",
    "region": "",
    "tags": [],
    "url": "https://github.com/ZrrSkywalker/LLaMA-Adapter",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 10,
    "name": "Alpaca-CoT",
    "vendor": "PhoebusSi",
    "intro": "extend CoT data to Alpaca to boost its reasoning ability.<br />aims to build an instruction finetuning (IFT) platform with extensive instruction collection (especially the CoT datasets)<br /> and a unified interface for various large language models.",
    "region": "",
    "tags": [],
    "url": "https://github.com/PhoebusSi/Alpaca-CoT",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 11,
    "name": "Llama-X",
    "vendor": "AetherCortex",
    "intro": "Open Academic Research on Improving LLaMA to SOTA LLM",
    "region": "",
    "tags": [],
    "url": "https://github.com/AetherCortex/Llama-X",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 12,
    "name": "OpenChatKit",
    "vendor": "TogetherComputer",
    "intro": "OpenChatKit provides a powerful, open-source base to create both specialized and general purpose chatbots for various applications.<br /> The kit includes an instruction-tuned language models, a moderation model, and an extensible retrieval system for including <br />up-to-date responses from custom repositories.",
    "region": "",
    "tags": [],
    "url": "https://github.com/togethercomputer/OpenChatKit",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 13,
    "name": "GPT4All",
    "vendor": "nomic-ai",
    "intro": "trained on a massive collection of clean assistant data including code, stories and dialogue",
    "region": "",
    "tags": [],
    "url": "https://github.com/nomic-ai/gpt4all",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 14,
    "name": "Chinese-LLaMA-Alpaca",
    "vendor": "@ymcui",
    "intro": "expand the Chinese vocabulary based on the original LLaMA and use Chinese data for secondary pre-training,<br /> further enhancing Chinese basic semantic understanding. Additionally, the project uses Chinese instruction data<br /> for fine-tuning on the basis of the Chinese LLaMA, significantly improving the model's understanding and execution of instructions.",
    "region": "",
    "tags": [],
    "url": "https://github.com/ymcui/Chinese-LLaMA-Alpaca",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 15,
    "name": "Vicuna",
    "vendor": "UC Berkley<br />Stanford<br />CMU",
    "intro": "Impressing GPT-4 with 90% ChatGPT Quality",
    "region": "",
    "tags": [],
    "url": "https://github.com/lm-sys/FastChat",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 16,
    "name": "bloomz.cpp",
    "vendor": "@NouamaneTazi",
    "intro": "C++ implementation for BLOOM inference.",
    "region": "",
    "tags": [],
    "url": "https://github.com/NouamaneTazi/bloomz.cpp",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 17,
    "name": "LMFlow",
    "vendor": "HKUST",
    "intro": "LMFlow is an extensible, convenient, and efficient toolbox for finetuning large machine learning models, designed to be user-friendly,<br /> speedy and reliable, and accessible to the entire community.<br />RAFT is a new alignment algorithm, which is more efficient than conventional (PPO-based) RLHF.",
    "region": "",
    "tags": [],
    "url": "https://github.com/OptimalScale/LMFlow",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 18,
    "name": "Cerebras-GPT",
    "vendor": "[Cerebras Systems](https://www.cerebras.net/)",
    "intro": "Pretrained LLM, GPT-3 like, Commercially available, efficiently trained on the[Andromeda](https://www.cerebras.net/andromeda/) AI supercomputer,<br />trained in accordance with [Chinchilla scaling laws](https://arxiv.org/abs/2203.15556) (20 tokens per model parameter) which is compute-optimal.",
    "region": "",
    "tags": [],
    "url": "https://huggingface.co/cerebras/Cerebras-GPT-13B",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 19,
    "name": "ChatDoctor",
    "vendor": "UT Southwestern/<br />UIUC/OSU/HDU",
    "intro": "Maybe the first domain-specific chat model tuned on LLaMA.",
    "region": "",
    "tags": [],
    "url": "https://github.com/Kent0n-Li/ChatDoctor",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 20,
    "name": "Open Assistant",
    "vendor": "LAION-AI",
    "intro": "a project meant to give everyone access to a great chat based large language model.",
    "region": "",
    "tags": [],
    "url": "https://github.com/LAION-AI/Open-Assistant",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 21,
    "name": "baize",
    "vendor": "UCSD/SYSU",
    "intro": "fine-tuned with[LoRA](https://github.com/microsoft/LoRA). It uses 100k dialogs generated by letting ChatGPT chat with itself. <br />Alpaca's data is also used to improve its performance.",
    "region": "",
    "tags": [],
    "url": "https://github.com/project-baize/baize",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 22,
    "name": "Koala",
    "vendor": "UC Berkley",
    "intro": "Rather than maximizing*quantity* by scraping as much web data as possible, the team focus on collecting a small *high-quality* dataset.",
    "region": "",
    "tags": [],
    "url": "https://github.com/young-geng/EasyLM",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 23,
    "name": "langchain-ChatGLM",
    "vendor": "@imClumsyPanda",
    "intro": "local knowledge based ChatGLM with langchain.",
    "region": "",
    "tags": [],
    "url": "https://github.com/imClumsyPanda/langchain-ChatGLM",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 24,
    "name": "Firefly",
    "vendor": "@yangjianxin1",
    "intro": "Instruction Tuning on Chinese dataset. Vocabulary pruning, ZeRO, and tensor parallelism<br /> are used to effectively reduce memory consumption and improve training efficiency.",
    "region": "",
    "tags": [],
    "url": "https://github.com/yangjianxin1/Firefly",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 25,
    "name": "GPT-4-LLM",
    "vendor": "microsoft",
    "intro": "aims to share data generated by GPT-4 for building an instruction-following LLMs with supervised learning and reinforcement learning.",
    "region": "",
    "tags": [],
    "url": "https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 26,
    "name": "pythia",
    "vendor": "EleutherAI",
    "intro": "combine interpretability analysis and scaling laws to understand how knowledge develops<br /> and evolves during training in autoregressive transformers.",
    "region": "",
    "tags": [],
    "url": "https://github.com/EleutherAI/pythia",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 27,
    "name": "StackLLaMA",
    "vendor": "Hugging Face",
    "intro": "trained on StackExchange data and the main goal is to serve as a tutorial and walkthrough on<br /> how to train model with RLHF and not primarily model performance.",
    "region": "",
    "tags": [],
    "url": "https://huggingface.co/trl-lib/llama-7b-se-rl-peft",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 28,
    "name": "ChatLLaMA",
    "vendor": "Nebuly",
    "intro": "a library that allows you to create hyper-personalized ChatGPT-like assistants using your own data and the least amount of compute possible.",
    "region": "",
    "tags": [],
    "url": "https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/chatllam",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 29,
    "name": "ChatLLaMA",
    "vendor": "@juncongmoo",
    "intro": "LLaMA-based RLHF model, runnable in a single GPU.",
    "region": "",
    "tags": [],
    "url": "https://github.com/juncongmoo/chatllama",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 30,
    "name": "minichatgpt",
    "vendor": "@juncongmoo",
    "intro": "To Train ChatGPT In 5 Minutes with ColossalAI.",
    "region": "",
    "tags": [],
    "url": "https://github.com/juncongmoo/minichatgpt",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 31,
    "name": "Luotuo-Chinese-LLM",
    "vendor": "@LC1332",
    "intro": "Instruction fine-tuned Chinese Language Models, with colab provided!",
    "region": "",
    "tags": [],
    "url": "https://github.com/LC1332/Luotuo-Chinese-LLM",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 32,
    "name": "Chinese-Vicuna",
    "vendor": "@Facico",
    "intro": "A Chinese Instruction-following LLaMA-based Model, fine-tuned with Lora, cpp inference supported, colab provided.",
    "region": "",
    "tags": [],
    "url": "https://github.com/Facico/Chinese-Vicuna",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 33,
    "name": "InstructGLM",
    "vendor": "@yanqiangmiffy",
    "intro": "ChatGLM based instruction-following model, fine-tuned on a variety of data sources, supports deepspeed accelerating and LoRA.",
    "region": "",
    "tags": [],
    "url": "https://github.com/yanqiangmiffy/InstructGLM",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 34,
    "name": "Wombat",
    "vendor": "alibaba",
    "intro": "a novel learning paradigm called RRHF, as an alternative of RLHF,  is proposed, which scores responses generated by<br /> different sampling policies and learns to align them with human preferences through ranking loss. And the performance<br />is comparable to RLHF, with less models used in the process.",
    "region": "",
    "tags": [],
    "url": "https://github.com/GanjinZero/RRHF",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 35,
    "name": "deepspeed-chat",
    "vendor": "microsoft",
    "intro": "Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales.",
    "region": "",
    "tags": [],
    "url": "https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 36,
    "name": "alpaca-glassoff",
    "vendor": "@WuJunde",
    "intro": "a mini image-acceptable Chat AI can run on your own laptop,  based on[stanford-alpaca](https://github.com/tatsu-lab/stanford_alpaca) and [alpaca-lora](https://github.com/tloen/alpaca-lora).",
    "region": "",
    "tags": [],
    "url": "https://github.com/WuJunde/alpaca-glassoff",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 37,
    "name": "Visual Med-Alpaca",
    "vendor": "Cambridge",
    "intro": "a multi-modal foundation model designed specifically for the biomedical domain",
    "region": "",
    "tags": [],
    "url": "https://github.com/cambridgeltl/visual-med-alpaca",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 38,
    "name": "Guanaco",
    "vendor": "@JosephusCheung",
    "intro": "A Multilingual Instruction-Following Language Model",
    "region": "",
    "tags": [],
    "url": "https://huggingface.co/datasets/JosephusCheung/GuanacoDataset",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 39,
    "name": "CAMEL",
    "vendor": "KAUST",
    "intro": "a novel communicative agent framework named*role-playing,* using *inception prompting* to<br /> guide chat agents toward task completion while maintaining consistency with human intentions.",
    "region": "",
    "tags": [],
    "url": "https://github.com/lightaime/camel",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 40,
    "name": "IDPChat",
    "vendor": "BaihaiAI",
    "intro": "Chinese multi-modal model, single GPU runnable, easy to deploy, UI provided.",
    "region": "",
    "tags": [],
    "url": "https://github.com/BaihaiAI/IDPChat",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 41,
    "name": "ChatRWKV",
    "vendor": "BlinkDL",
    "intro": "powered by RWKV (**100% RNN)**, Training sponsored by Stability EleutherAI.",
    "region": "",
    "tags": [],
    "url": "https://github.com/BlinkDL/ChatRWKV",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 42,
    "name": "LLM Zoo",
    "vendor": "@FreedomIntelligence",
    "intro": "a project that provides data, models, and evaluation benchmark for large language models.<br />model released: Phoenix, Chimera",
    "region": "",
    "tags": [],
    "url": "https://github.com/FreedomIntelligence/LLMZoo",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 43,
    "name": "MiniGPT-4",
    "vendor": "KAUST",
    "intro": "MiniGPT-4 aligns a frozen visual encoder from BLIP-2 with a frozen LLM, Vicuna, using just one projection layer,<br /> and yields many emerging vision-language capabilities similar to those demonstrated in GPT-4.",
    "region": "",
    "tags": [],
    "url": "https://github.com/Vision-CAIR/MiniGPT-4",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 44,
    "name": "Huatuo",
    "vendor": "HIT",
    "intro": "fine-tuned with Chinese medical knowledge dataset, which is generated by using gpt3.5 api.",
    "region": "",
    "tags": [],
    "url": "https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 45,
    "name": "LLaVA",
    "vendor": "UWâ€“Madison/MSR<br />/Columbia University",
    "intro": "visual instruction tuning is proposed, towards building large language and vision models with GPT-4 level capabilities.",
    "region": "",
    "tags": [],
    "url": "https://github.com/haotian-liu/LLaVA",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 46,
    "name": "StableLM",
    "vendor": "Stability-AI",
    "intro": "Stability AI Language Models.",
    "region": "",
    "tags": [],
    "url": "https://github.com/Stability-AI/StableLM",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 47,
    "name": "DoctorGLM",
    "vendor": "ShanghaiTech, etc",
    "intro": "Chinese medical consultation model fine-tuned on ChatGLM-6B.",
    "region": "",
    "tags": [],
    "url": "https://github.com/xionghonglin/DoctorGLM",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 48,
    "name": "RedPajama-Data",
    "vendor": "TogetherComputer",
    "intro": "An Open Source Recipe to Reproduce LLaMA training dataset.",
    "region": "",
    "tags": [],
    "url": "https://github.com/togethercomputer/RedPajama-Data",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 49,
    "name": "MOSS",
    "vendor": "FDU",
    "intro": "An open-source tool-augmented conversational language model from Fudan University.",
    "region": "",
    "tags": [],
    "url": "https://github.com/OpenLMLab/MOSS",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 50,
    "name": "BBT-2",
    "vendor": "ssymmetry & FDU",
    "intro": "120B open-source LM.",
    "region": "",
    "tags": [],
    "url": "https://bbt.ssymmetry.com/",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  },
  {
    "id": 51,
    "name": "BioMedGPT-1.6B",
    "vendor": "Tsinghua AIR",
    "intro": "a pre-trained multi-modal molecular foundation model with 1.6B parameters that associates 2D molecular graphs with texts.",
    "region": "",
    "tags": [],
    "url": "https://github.com/BioFM/OpenBioMed",
    "icon": "",
    "publish_date": "2023-04-23",
    "voters": [],
    "vote_count": 0,
    "voted": false
  }
]